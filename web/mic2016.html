<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" type="text/css" href="style.css" media="screen"/>
<link rel="stylesheet" type="text/css" href="mic2016.css" media="screen"/>
	<title>UK Music Informatics and Cognition (MIC) 2016 Workshop</title>
</head>
<body>

	<div id="main">
		<div id="top">
			<a class="name" href="mic2016.html">UK Music Informatics and Cognition (MIC) 2016 Workshop</a>
		</div>
    
		<div id="middle">
			<div id="navigation">
				<ul id="nav-list">
					<li class="nav nav-active">More Coming Soon</li>
				</ul>
			</div>
			
			<div id="content">
				<div class="section-header">Call For Participation</div>
		    
		    <table>
		    	<tr><td>What: 1st UK Music Informatics and Cognition (MIC) workshop</td></tr>
				<tr><td>When: Wednesday 20th July, 1:00pm - 7:00pm</td></tr>
				<tr><td>Where: University of Edinburgh, Informatics Forum</td></tr>
			</table>
						
			<h4>Schedule</h4>
			<table>
				<tr><td>1:00-1:15 - Room 4.31 - Introduction, Mark Steedman</td></tr>
		    	<tr><td>1:15-5:00 - Room 4.31 - Invited talks (speakers and abstracts below, specific timing TBA)</td></tr>
		    	<tr><td>5:00-7:00 - Miniforum 2 - Poster session and reception</td></tr>
		    </table>
		    <br>
			
			<h4>Topics include (but are not limited to)</h4>
			<ul>
			<li>Data-driven methods for music analysis</li>
			<li>Music signal processing</li>
			<li>Symbolic music processing</li>
			<li>Music information retrieval</li>
			<li>Computational music analysis</li>
			<li>Music cognition</li>
			<li>Music and language</li>
			</ul>
			<br>
			
			<p>
			Computational methods play an important role in modeling human cognitive processing of music, as well as
			unlocking practical applications such as music retrieval and transcription. This workshop, focused on
			data-driven methods for music analysis, brings together UK researchers to share different perspectives
			and disciplinary approaches to music in a computational and/or cognitive context. The proximity of these
			universities makes this a great opportunity for researchers in the area to meet.
			</p>
			
			<p>
			If you would like to present a poster, please email <a href="mailto:A.McLeod-5@sms.ed.ac.uk">A.McLeod-5@sms.ed.ac.uk</a>
			with the tile, abstract, and a pdf of the poster (size A0) by Monday 18th July at 5:00pm.
			We will print the posters for you.
			</p>
			
			<p>
			Attendance is free, but please
			<a href="https://www.eventbrite.co.uk/e/uk-music-informatics-and-cognition-mic-2016-workshop-tickets-26566583366">sign up</a>
			with the eventbrite link provided so we have an idea
			of the number of people to expect.
			</p>
			
			<p>
			Hope to see you all there!
			<br>
			Andrew McLeod
			</p>
			
			<br><br>
			
			<h3>Speakers and Abstracts</h3>
			
			<p>
			<a href="http://www.eecs.qmul.ac.uk/~emmanouilb/">Emmanouil Benetos</a> - Queen Mary University
			</p>
			
			<p>
			Title: Automatic music transcription using matrix decomposition methods
			</p>
			
			<p>
			Abstract:
			Automatic music transcription (AMT) is defined as the process of converting an acoustic music signal into some form of human- or machine-readable musical notation. It is considered a key enabling technology in the field of music information retrieval and can be divided into several subtasks, which include multi-pitch detection, note onset/offset detection, instrument recognition, pitch/timing quantisation, extraction of rhythmic information, and extraction of dynamics and expressive information. However, despite recent advances AMT still remains an open problem, especially when considering multiple-instrument music with high polyphony.
			</p>
			
			<p>
			A large part of current AMT research focuses on matrix decomposition methods, which decompose a time-frequency representation of a music signal into a series of note templates and note activations. This has led to music transcription systems that are computationally efficient, robust, and interpretable. In this talk, I will present recent advances in AMT focusing on proposed systems that are able to detect multiple pitches and instruments, are able to support tuning changes and frequency modulations, and take into account the temporal evolution of notes. Recent work on integrating music language models with acoustic models for improving AMT performance will also be discussed, along with applications of AMT to fields beyond music informatics, including musicology, music acoustics, music education, and environmental sound analysis.
			</p>
			
			<br>
			<p>
			<a href="http://cogsci.ucmerced.edu/shih/">Stephanie Shih</a> - UC Merced
			</p>
			
			<p>
			TBA
			</p>
			
			<br>
			<p>
			<a href="http://www.staff.city.ac.uk/~sa746/">Tillman Weyde</a> - City University of London
			</p>
			
			<p>
			TBA
			</p>
			
			<br>
			<p>
			<a href="http://homepages.inf.ed.ac.uk/s1331854/home.html">Andrew McLeod</a> - University of Edinburgh
			</p>
			
			<p>
			Title: Symbolic Music Analysis for Music Transcription
			</p>
			
			<p>
			Abstract:
			Automatic Music Transcription (AMT) is commonly defined as the
			process of converting an acoustic musical signal into some form of musical
			notation, often a musical score. In this presentation, I argue that this
			problem cannot be sufficiently solved without the use of
			some sort of music language model. I will show that the use of Natural
			Language Processing (NLP) techniques can aid in the creation of this
			music language model when applied to a symbolic music representation
			such as MIDI performance data.
			</p>
			
			<p>
			I first present a working Voice Separation model which works on symbolic
			music data, both quantized and performed. I also discuss ongoing work
			on time signature identification based on rhythmic analysis using a lexicalized
			probabilistic context-free grammar, and argue that such a grammar can capture
			the long-range dependencies of musical rhythms.
		    </p>
			</div>
		</div>
    
		<div id="middle-bottom-border"></div>
   
		<div id="bottom">
			Sponsored by <a href="http://www.ilcc.inf.ed.ac.uk/">ILCC</a>
		</div>
	</div>

</body>
</html>